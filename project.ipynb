{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler((0,1))\n",
    "scaler.fit(X)\n",
    "X_t = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf = pd.DataFrame(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_t, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sk_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "preds = sk_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",accuracy_score(y_test, preds))\n",
    "print(\"F1 Score: \",f1_score(y_test, preds))\n",
    "print(\"Precision: \",precision_score(y_test, preds))\n",
    "print(\"Recall: \",recall_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.power(np.e,-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approxsigmoid(z):\n",
    "    return 1 / (1 + np.power(2,-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TaylorSigmoid(z):\n",
    "    return (0.5+0.25*z-np.power(z, 3)/48.+np.power(z,5)/480.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-2,2,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, sigmoid(t),)\n",
    "plt.title(\"Sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, approxsigmoid(t))\n",
    "plt.title(\"e=>2 Approximation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, TaylorSigmoid(t))\n",
    "plt.title(\"Taylor Approximation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "res = sigmoid(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ares = approxsigmoid(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "tres = TaylorSigmoid(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sigmoid(t)\n",
    "ares = approxsigmoid(t)\n",
    "tres = TaylorSigmoid(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taylor Approximation Error: \", mean_absolute_error(tres,res))\n",
    "print(\"e Approximation Error: \", mean_absolute_error(ares,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    # defining parameters such as learning rate, number ot iterations, whether to include intercept, \n",
    "    # and verbose which says whether to print anything or not like, loss etc.\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=50000, fit_intercept=True, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    # function to define the Incercept value.\n",
    "    def __b_intercept(self, X):\n",
    "        # initially we set it as all 1's\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        # then we concatinate them to the value of X, we don't add we just append them at the end.\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid_function(self, z):\n",
    "        # this is our actual sigmoid function which predicts our yp\n",
    "        return 1 / (1 + np.power(np.e,-z))\n",
    "    \n",
    "    def __loss(self, yp, y):\n",
    "        # this is the loss function which we use to minimize the error of our model\n",
    "        return (-y * np.log(yp) - (1 - y) * np.log(1 - yp)).mean()\n",
    "    \n",
    "    # this is the function which trains our model.\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # as said if we want our intercept term to be added we use fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X = self.__b_intercept(X)\n",
    "        \n",
    "        # weights initialization of our Normal Vector, initially we set it to 0, then we learn it eventually\n",
    "        self.W = np.zeros(X.shape[1])\n",
    "        \n",
    "        # this for loop runs for the number of iterations provided\n",
    "        for i in range(self.num_iterations):\n",
    "            \n",
    "            # this is our W * Xi\n",
    "            z = np.dot(X, self.W)\n",
    "            \n",
    "            # this is where we predict the values of Y based on W and Xi\n",
    "            yp = self.__sigmoid_function(z)\n",
    "            \n",
    "            # this is where the gradient is calculated form the error generated by our model\n",
    "            gradient = np.dot(X.T, (yp - y)) / y.size\n",
    "            \n",
    "            # this is where we update our values of W, so that we can use the new values for the next iteration\n",
    "            self.W -= self.learning_rate * gradient\n",
    "            \n",
    "            # this is our new W * Xi\n",
    "            z = np.dot(X, self.W)\n",
    "            yp = self.__sigmoid_function(z)\n",
    "            \n",
    "            # this is where the loss is calculated\n",
    "            loss = self.__loss(yp, y)\n",
    "            \n",
    "            # as mentioned above if we want to print somehting we use verbose, so if verbose=True then our loss get printed\n",
    "            if(self.verbose ==True and i % 5000 == 0):\n",
    "                print(f'loss: {loss} \\t')\n",
    "    \n",
    "    # this is where we predict the probability values based on out generated W values out of all those iterations.\n",
    "    def predict_prob(self, X):\n",
    "        # as said if we want our intercept term to be added we use fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X = self.__b_intercept(X)\n",
    "        \n",
    "        # this is the final prediction that is generated based on the values learned.\n",
    "        return self.__sigmoid_function(np.dot(X, self.W))\n",
    "    \n",
    "    # this is where we predict the actual values 0 or 1 using round. anything less than 0.5 = 0 or more than 0.5 is 1\n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model = LogisticRegression(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scratch_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_loss = [0.6802027676278086,0.1437848188409872,0.12231158389813686,0.11103282957702557,0.10352850820742168,\n",
    "                0.09804229644421379,0.09380104663005029,0.09038101808681631,0.08753083896144674,0.08509315759390447 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scratch_lr_preds = scratch_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",accuracy_score(y_test, scratch_lr_preds))\n",
    "print(\"F1 Score: \",f1_score(y_test, scratch_lr_preds))\n",
    "print(\"Precision: \",precision_score(y_test, scratch_lr_preds))\n",
    "print(\"Recall: \",recall_score(y_test, scratch_lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproximateRegression:\n",
    "    \n",
    "    # defining parameters such as learning rate, number ot iterations, whether to include intercept, \n",
    "    # and verbose which says whether to print anything or not like, loss etc.\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=50000, fit_intercept=True, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    # function to define the Incercept value.\n",
    "    def __b_intercept(self, X):\n",
    "        # initially we set it as all 1's\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        # then we concatinate them to the value of X, we don't add we just append them at the end.\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid_function(self, z):\n",
    "        # this is our actual sigmoid function which predicts our yp\n",
    "        return 1 / (1 + np.power(2,-z))\n",
    "    \n",
    "    def __loss(self, yp, y):\n",
    "        # this is the loss function which we use to minimize the error of our model\n",
    "        return (-y * np.log(yp) - (1 - y) * np.log(1 - yp)).mean()\n",
    "    \n",
    "    # this is the function which trains our model.\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # as said if we want our intercept term to be added we use fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X = self.__b_intercept(X)\n",
    "        \n",
    "        # weights initialization of our Normal Vector, initially we set it to 0, then we learn it eventually\n",
    "        self.W = np.zeros(X.shape[1])\n",
    "        \n",
    "        # this for loop runs for the number of iterations provided\n",
    "        for i in range(self.num_iterations):\n",
    "            \n",
    "            # this is our W * Xi\n",
    "            z = np.dot(X, self.W)\n",
    "            \n",
    "            # this is where we predict the values of Y based on W and Xi\n",
    "            yp = self.__sigmoid_function(z)\n",
    "            \n",
    "            # this is where the gradient is calculated form the error generated by our model\n",
    "            gradient = np.dot(X.T, (yp - y)) / y.size\n",
    "            \n",
    "            # this is where we update our values of W, so that we can use the new values for the next iteration\n",
    "            self.W -= self.learning_rate * gradient\n",
    "            \n",
    "            # this is our new W * Xi\n",
    "            z = np.dot(X, self.W)\n",
    "            yp = self.__sigmoid_function(z)\n",
    "            \n",
    "            # this is where the loss is calculated\n",
    "            loss = self.__loss(yp, y)\n",
    "            \n",
    "            # as mentioned above if we want to print somehting we use verbose, so if verbose=True then our loss get printed\n",
    "            if(self.verbose ==True and i % 5000 == 0):\n",
    "                print(f'loss: {loss} \\t')\n",
    "\n",
    "    \n",
    "    # this is where we predict the probability values based on out generated W values out of all those iterations.\n",
    "    def predict_prob(self, X):\n",
    "        # as said if we want our intercept term to be added we use fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X = self.__b_intercept(X)\n",
    "        \n",
    "        # this is the final prediction that is generated based on the values learned.\n",
    "        return self.__sigmoid_function(np.dot(X, self.W))\n",
    "    \n",
    "    # this is where we predict the actual values 0 or 1 using round. anything less than 0.5 = 0 or more than 0.5 is 1\n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_model = ApproximateRegression(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "approx_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_loss = [0.6838808388499469,0.15668311336660568,0.13327044900788637,0.12119362980100588,0.11316384088637117,\n",
    "              0.10721375561196354,0.10254874031747165,0.09876261054734893,0.09560845558233506,0.09292325821026125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "approx_lr_preds = approx_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",accuracy_score(y_test, approx_lr_preds))\n",
    "print(\"F1 Score: \",f1_score(y_test, approx_lr_preds))\n",
    "print(\"Precision: \",precision_score(y_test, approx_lr_preds))\n",
    "print(\"Recall: \",recall_score(y_test, approx_lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaylorRegression:\n",
    "    \n",
    "    # defining parameters such as learning rate, number ot iterations, whether to include intercept, \n",
    "    # and verbose which says whether to print anything or not like, loss etc.\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=50000, fit_intercept=True, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    # function to define the Incercept value.\n",
    "    def __b_intercept(self, X):\n",
    "        # initially we set it as all 1's\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        # then we concatinate them to the value of X, we don't add we just append them at the end.\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid_function(self, z):\n",
    "        # this is our actual sigmoid function which predicts our yp\n",
    "        res = (0.5+0.25*z-np.power(z,3)/48 + np.power(z,5)/480)\n",
    "        return np.nan_to_num(res)\n",
    "    \n",
    "    def __loss(self, yp, y):\n",
    "        # this is the loss function which we use to minimize the error of our model\n",
    "        return np.nan_to_num((-y * np.log(yp) - (1 - y) * np.log(1 - yp))).mean()\n",
    "    \n",
    "    # this is the function which trains our model.\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # as said if we want our intercept term to be added we use fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X = self.__b_intercept(X)\n",
    "        \n",
    "        # weights initialization of our Normal Vector, initially we set it to 0, then we learn it eventually\n",
    "        self.W = np.zeros(X.shape[1])\n",
    "        \n",
    "        # this for loop runs for the number of iterations provided\n",
    "        for i in range(self.num_iterations):\n",
    "            \n",
    "            # this is our W * Xi\n",
    "            z = np.dot(X, self.W)\n",
    "            \n",
    "            # this is where we predict the values of Y based on W and Xi\n",
    "            yp = self.__sigmoid_function(z)\n",
    "            \n",
    "   \n",
    "            \n",
    "            # this is where the gradient is calculated form the error generated by our model\n",
    "            gradient = np.dot(X.T, (yp - y)) / y.size\n",
    "            \n",
    "            # this is where we update our values of W, so that we can use the new values for the next iteration\n",
    "            self.W -= self.learning_rate * gradient\n",
    "            \n",
    "            # this is our new W * Xi\n",
    "            z = np.dot(X, self.W)\n",
    "            yp = self.__sigmoid_function(z)\n",
    " \n",
    "            \n",
    "            # this is where the loss is calculated\n",
    "            loss = self.__loss(yp, y)\n",
    "            #print(\"loss: \", loss)\n",
    "            \n",
    "            # as mentioned above if we want to print somehting we use verbose, so if verbose=True then our loss get printed\n",
    "            if(self.verbose ==True and i % 5000 == 0):\n",
    "                print(f'loss: {loss} \\t')\n",
    "                \n",
    "\n",
    "    \n",
    "    # this is where we predict the probability values based on out generated W values out of all those iterations.\n",
    "    def predict_prob(self, X):\n",
    "        # as said if we want our intercept term to be added we use fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X = self.__b_intercept(X)\n",
    "        \n",
    "        # this is the final prediction that is generated based on the values learned.\n",
    "        return self.__sigmoid_function(np.dot(X, self.W))\n",
    "    \n",
    "    # this is where we predict the actual values 0 or 1 using round. anything less than 0.5 = 0 or more than 0.5 is 1\n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_model = TaylorRegression(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taylor_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_loss = [0.6802027675850325, 0.24888153461793144,0.23565447667633324, 0.22783043968681171,0.22211908628137508,\n",
    "              0.21780747243903473, 0.21434131655968433, 0.2115019167958568, 0.2091635430385828, 0.2071597232944494]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "taylor_lr_preds = taylor_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_lr_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(taylor_lr_preds)):\n",
    "    if taylor_lr_preds[i]<0:\n",
    "        taylor_lr_preds[i] = 0\n",
    "    elif taylor_lr_preds[i]>1:\n",
    "        taylor_lr_preds[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taylor_lr_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \",accuracy_score(y_test, taylor_lr_preds))\n",
    "print(\"F1 Score: \",f1_score(y_test, taylor_lr_preds))\n",
    "print(\"Precision: \",precision_score(y_test, taylor_lr_preds))\n",
    "print(\"Recall: \",recall_score(y_test, taylor_lr_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scratch_loss, label=\"original\")\n",
    "plt.plot(approx_loss, label=\"e=>2 approx\")\n",
    "plt.plot(taylor_loss, label=\"taylor approx\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
